{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfication model using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('finaldataset.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>USGS reports a M1.7 #earthquake 70km ESE of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>QuakeFactor M 3.0, Southern Alaska: Sunday, Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>What you wanna say abt #TTP #PeaceTalks &amp;amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Top Story: Pakistan quake survivors face long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @SaimaMohsin: 264 dead, more than 400 injur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                         tweet_text\n",
       "0           0      4  USGS reports a M1.7 #earthquake 70km ESE of He...\n",
       "1           1      3  QuakeFactor M 3.0, Southern Alaska: Sunday, Se...\n",
       "2           2      3  What you wanna say abt #TTP #PeaceTalks &amp; ...\n",
       "3           3      1  Top Story: Pakistan quake survivors face long ...\n",
       "4           4      2  RT @SaimaMohsin: 264 dead, more than 400 injur..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5132</td>\n",
       "      <td>4741</td>\n",
       "      <td>RT @Lunchboxhero45: Hurricane Odile.. Please s...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5884</td>\n",
       "      <td>5261</td>\n",
       "      <td>RT @PzFeed: This is the tragic sight of Los Ca...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9250</td>\n",
       "      <td>8805</td>\n",
       "      <td>RT @bgittleson: This photo is very California....</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7471</td>\n",
       "      <td>6892</td>\n",
       "      <td>RT @RedCross: To check on loved ones in #Baja ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_text                                                            \\\n",
       "           count unique                                                top   \n",
       "label                                                                        \n",
       "1           5132   4741  RT @Lunchboxhero45: Hurricane Odile.. Please s...   \n",
       "2           5884   5261  RT @PzFeed: This is the tragic sight of Los Ca...   \n",
       "3           9250   8805  RT @bgittleson: This photo is very California....   \n",
       "4           7471   6892  RT @RedCross: To check on loved ones in #Baja ...   \n",
       "\n",
       "            \n",
       "      freq  \n",
       "label       \n",
       "1       24  \n",
       "2       54  \n",
       "3       27  \n",
       "4       24  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'tweet_text'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import re\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer=PorterStemmer()\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    word = word.strip('\\'\"?!,.():;')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    return word\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_tweet(tweet):\n",
    "    processed_tweet = []\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    # Replaces URLs with the word URL\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', 'URL', tweet)\n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # Replaces #hashtag with hashtag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # Strip space, \" and ' from tweet\n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    #tweet = handle_emojis(tweet)\n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    #stop = stopwords.words('english')\n",
    "    tokens = [word for sent in nltk.sent_tokenize(tweet) for word in nltk.word_tokenize(sent)]\n",
    "    #tokens = [token for token in tokens if token not in stop]\n",
    "    #tokens = [word for word in tokens if len(word) >= 2]\n",
    "    #tagged = nltk.tag.pos_tag(tokens)\n",
    "    #k=['NNP','NNPS']\n",
    "    #tokens =[word for word,tag in tagged if tag not in k]\n",
    "    tweet= ' '.join(tokens)\n",
    "    #words = tweet.split()\n",
    "\n",
    "    #for word in words:\n",
    "    #    word = preprocess_word(word)\n",
    "    #    if is_valid_word(word):\n",
    "    #        word = str(stemmer.stem(word))\n",
    "    #    processed_tweet.append(word)\n",
    "\n",
    "    #return ' '.join(processed_tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=0\n",
    "for i in df['tweet_text']:\n",
    "    df.at[g,'tweet_text']=preprocess_tweet(i)\n",
    "    g+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label']\n",
    "df=df.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(df['tweet_text'],y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22189,)\n",
      "(5548,)\n",
      "(22189,)\n",
      "(5548,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22189, 24323)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "#X_train_counts = count_vect.fit_transform(df2['tweet'])\n",
    "\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 22775)\t1\n",
      "  (0, 6324)\t1\n",
      "  (0, 23741)\t1\n",
      "  (0, 8948)\t1\n",
      "  (0, 24228)\t1\n",
      "  (0, 16358)\t1\n",
      "  (0, 23641)\t1\n",
      "  (0, 9701)\t1\n",
      "  (0, 6023)\t1\n",
      "  (0, 19961)\t1\n",
      "  (0, 23129)\t1\n",
      "  (0, 17510)\t1\n",
      "  (0, 14503)\t1\n",
      "  (0, 20096)\t1\n",
      "  (0, 10326)\t1\n",
      "  (0, 6010)\t1\n",
      "  (0, 4769)\t1\n",
      "  (0, 198)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22189, 24323)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017303532804614"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.score(X_test,Y_test) \n",
    "# Without using n-grams and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_WGS = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2),(1,3)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "GS_clf = GS_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_GS=GS_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8229992790194665"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_clf.score(X_test,Y_test)\n",
    "# After applying Grid-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pone=0\n",
    "ptwo=0\n",
    "pthree =0\n",
    "pfour=0\n",
    "for i in predicted_WGS:\n",
    "    if i==1:\n",
    "        pone =pone+1\n",
    "    if i==2:\n",
    "        ptwo =ptwo+1\n",
    "    if i==3:\n",
    "        pthree=pthree+1\n",
    "    if i==4:\n",
    "        pfour=pfour+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "one=0\n",
    "two=0\n",
    "three =0\n",
    "four=0\n",
    "for i in Y_test:\n",
    "    if i==1:\n",
    "        one=one+1\n",
    "    if i==2:\n",
    "        two=two+1\n",
    "    if i==3:\n",
    "        three=three+1\n",
    "    if i==4:\n",
    "        four=four+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acutal values vs. Predicted value without Gridsearch\n",
      "989 \t 926 \t 0.9362992922143579\n",
      "1174 \t 1100 \t 0.9369676320272572\n",
      "1862 \t 1948 \t 1.046186895810956\n",
      "1523 \t 1574 \t 1.0334865397242285\n"
     ]
    }
   ],
   "source": [
    "print(\"Acutal values vs. Predicted value without Gridsearch\")\n",
    "print(one,\"\\t\", pone,'\\t',pone/one)\n",
    "print(two,\"\\t\", ptwo,'\\t',ptwo/two)\n",
    "print(three,\"\\t\", pthree,'\\t',pthree/three)\n",
    "print(four,\"\\t\", pfour,'\\t',pfour/four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pone=0\n",
    "ptwo=0\n",
    "pthree =0\n",
    "pfour=0\n",
    "for i in predicted_GS:\n",
    "    if i==1:\n",
    "        pone =pone+1\n",
    "    if i==2:\n",
    "        ptwo =ptwo+1\n",
    "    if i==3:\n",
    "        pthree=pthree+1\n",
    "    if i==4:\n",
    "        pfour=pfour+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acutal values vs. Predicted value with Gridsearch\n",
      "989 \t 1131 \t 1.1435793731041457\n",
      "1174 \t 1188 \t 1.0119250425894377\n",
      "1862 \t 1812 \t 0.9731471535982814\n",
      "1523 \t 1417 \t 0.9304005252790545\n"
     ]
    }
   ],
   "source": [
    "print(\"Acutal values vs. Predicted value with Gridsearch\")\n",
    "print(one,\"\\t\", pone,'\\t',pone/one)\n",
    "print(two,\"\\t\", ptwo,'\\t',ptwo/two)\n",
    "print(three,\"\\t\", pthree,'\\t',pthree/three)\n",
    "print(four,\"\\t\", pfour,'\\t',pfour/four)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
